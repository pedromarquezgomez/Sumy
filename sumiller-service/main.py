#!/usr/bin/env python3
"""
Sumiller Service - Microservicio Aut√≥nomo con Filtro Inteligente
Sumiller inteligente con memoria integrada, b√∫squeda de vinos y filtro LLM.
"""

import os
import json
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime

from fastapi import FastAPI, HTTPException, Body
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import httpx
from openai import AsyncOpenAI
from dotenv import load_dotenv

# Cargar variables de entorno desde .env (solo en desarrollo)
load_dotenv()

# Importar memoria integrada y filtro inteligente
from memory import memory, SumillerMemory
from query_filter import filter_and_classify_query, CATEGORY_RESPONSES

# Configuraci√≥n de logging
log_level = os.getenv("LOG_LEVEL", "INFO")
logging.basicConfig(level=getattr(logging, log_level.upper()))
logger = logging.getLogger(__name__)

# Configuraci√≥n OpenAI
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

# Configuraci√≥n del entorno
ENVIRONMENT = os.getenv("ENVIRONMENT", "development")

# URL del servicio de b√∫squeda (opcional)
SEARCH_SERVICE_URL = os.getenv("SEARCH_SERVICE_URL", "")

# ‚ú® NUEVO: URL del servicio RAG para conocimiento profundo
RAG_SERVICE_URL = os.getenv("RAG_SERVICE_URL", "https://agentic-rag-service-597742621765.europe-west1.run.app")

# Configuraci√≥n del servidor
HOST = os.getenv("HOST", "0.0.0.0")
PORT = int(os.getenv("PORT", "8080"))

if not OPENAI_API_KEY:
    logger.warning("‚ö†Ô∏è OPENAI_API_KEY no configurada")
    openai_client = None
else:
    openai_client = AsyncOpenAI(
        api_key=OPENAI_API_KEY,
        base_url=OPENAI_BASE_URL,
        timeout=30.0  # Aumentar timeout a 30 segundos
    )
    logger.info(f"‚úÖ OpenAI configurado: {OPENAI_BASE_URL} - Modelo: {OPENAI_MODEL}")

# FastAPI App
app = FastAPI(
    title="Sumiller Service",
    description="Microservicio aut√≥nomo de sumiller con memoria integrada y filtro inteligente",
    version="2.0.0"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Modelos de datos
class QueryRequest(BaseModel):
    query: str
    user_id: str = "default_user"
    session_id: Optional[str] = None

class WineRatingRequest(BaseModel):
    wine_name: str
    rating: int  # 1-5
    notes: str = ""
    user_id: str = "default_user"

class PreferencesRequest(BaseModel):
    preferences: Dict[str, Any]
    user_id: str = "default_user"

class SumillerResponse(BaseModel):
    response: str
    wines_recommended: List[Dict[str, Any]] = []
    user_context: Dict[str, Any] = {}
    confidence: float = 0.8
    query_category: str = "unknown"  # NUEVO: categor√≠a de la consulta
    used_rag: bool = False          # NUEVO: si se us√≥ RAG o no

# Base de conocimientos de vinos (embebida)
WINE_KNOWLEDGE = [
    {
        "name": "Ribera del Duero Reserva",
        "type": "Tinto",
        "region": "Ribera del Duero",
        "grape": "Tempranillo",
        "price": 25.50,
        "pairing": "Carnes rojas, cordero, quesos curados",
        "description": "Vino tinto con crianza en barrica, taninos suaves y notas a frutos rojos.",
        "temperature": "16-18¬∞C"
    },
    {
        "name": "Albari√±o R√≠as Baixas",
        "type": "Blanco",
        "region": "R√≠as Baixas",
        "grape": "Albari√±o",
        "price": 18.90,
        "pairing": "Mariscos, pescados, paella",
        "description": "Vino blanco fresco con acidez equilibrada y notas c√≠tricas.",
        "temperature": "8-10¬∞C"
    },
    {
        "name": "Rioja Gran Reserva",
        "type": "Tinto",
        "region": "Rioja",
        "grape": "Tempranillo, Garnacha",
        "price": 45.00,
        "pairing": "Caza, carnes asadas, quesos a√±ejos",
        "description": "Vino tinto de larga crianza con complejidad arom√°tica excepcional.",
        "temperature": "17-19¬∞C"
    },
    {
        "name": "Cava Brut Nature",
        "type": "Espumoso",
        "region": "Pened√®s",
        "grape": "Macabeo, Xarel¬∑lo, Parellada",
        "price": 12.75,
        "pairing": "Aperitivos, mariscos, celebraciones",
        "description": "Espumoso elegante sin az√∫car a√±adido, burbujas finas y persistentes.",
        "temperature": "6-8¬∞C"
    }
]

# ‚ú® NUEVA FUNCI√ìN: Consultar RAG service para conocimiento profundo
async def query_rag_service(query: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
    """
    Consultar el servicio RAG para obtener conocimiento profundo sobre vinos y sumiller√≠a.
    """
    try:
        if not RAG_SERVICE_URL:
            logger.warning("‚ö†Ô∏è RAG_SERVICE_URL no configurada")
            return {"answer": "", "sources": [], "error": "RAG service not configured"}
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            payload = {
                "query": query,
                "context": context or {},
                "max_results": 3
            }
            
            logger.info(f"ü§ñ Consultando RAG service: {query}")
            response = await client.post(
                f"{RAG_SERVICE_URL}/query",
                json=payload
            )
            
            if response.status_code == 200:
                rag_data = response.json()
                logger.info(f"‚úÖ RAG response: {len(rag_data.get('sources', []))} fuentes encontradas")
                return rag_data
            else:
                logger.error(f"‚ùå RAG service error: {response.status_code} - {response.text}")
                return {"answer": "", "sources": [], "error": f"RAG service returned {response.status_code}"}
                
    except httpx.TimeoutException:
        logger.error("‚è∞ Timeout consultando RAG service")
        return {"answer": "", "sources": [], "error": "RAG service timeout"}
    except Exception as e:
        logger.error(f"‚ùå Error consultando RAG service: {e}")
        return {"answer": "", "sources": [], "error": str(e)}

async def search_wines(query: str, max_results: int = 3) -> List[Dict[str, Any]]:
    """B√∫squeda de vinos en la base de conocimientos local."""
    try:
        # Si hay servicio de b√∫squeda externo, usarlo
        if SEARCH_SERVICE_URL:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{SEARCH_SERVICE_URL}/search",
                    json={"query": query, "max_results": max_results},
                    timeout=10.0
                )
                if response.status_code == 200:
                    return response.json().get("wines", [])
        
        # B√∫squeda local simple
        query_lower = query.lower()
        results = []
        
        for wine in WINE_KNOWLEDGE:
            score = 0
            
            # Buscar en nombre
            if query_lower in wine["name"].lower():
                score += 3
            
            # Buscar en tipo
            if query_lower in wine["type"].lower():
                score += 2
            
            # Buscar en maridaje
            if query_lower in wine["pairing"].lower():
                score += 2
            
            # Buscar en regi√≥n
            if query_lower in wine["region"].lower():
                score += 1
            
            # Buscar en descripci√≥n
            if query_lower in wine["description"].lower():
                score += 1
            
            if score > 0:
                wine_result = wine.copy()
                wine_result["relevance_score"] = score / 10.0
                results.append(wine_result)
        
        # Ordenar por relevancia
        results.sort(key=lambda x: x["relevance_score"], reverse=True)
        return results[:max_results]
        
    except Exception as e:
        logger.error(f"Error en b√∫squeda de vinos: {e}")
        return []

async def generate_sumiller_response(
    query: str, 
    wines: List[Dict[str, Any]], 
    user_context: Dict[str, Any],
    category: str = "WINE_SEARCH"
) -> str:
    """Generar respuesta del sumiller usando OpenAI."""
    
    if not openai_client:
        # Respuesta fallback sin IA - M√ÅS COMPLETA
        if wines:
            # Seleccionar los mejores vinos (hasta 3)
            top_wines = wines[:3]
            response = "üç∑ **Recomendaciones de vinos:**\n\n"
            
            for i, wine in enumerate(top_wines, 1):
                name = wine.get("name", "Vino")
                price = wine.get("price", "N/A")
                region = wine.get("region", "")
                wine_type = wine.get("type", "")
                description = wine.get("description", "")
                pairing = wine.get("pairing", "")
                
                response += f"**{i}. {name}**\n"
                if wine_type:
                    response += f"   ‚Ä¢ Tipo: {wine_type}\n"
                if region:
                    response += f"   ‚Ä¢ Regi√≥n: {region}\n"
                if price != "N/A":
                    response += f"   ‚Ä¢ Precio: {price}‚Ç¨\n"
                if description:
                    response += f"   ‚Ä¢ Descripci√≥n: {description[:150]}{'...' if len(description) > 150 else ''}\n"
                if pairing:
                    response += f"   ‚Ä¢ Maridaje: {pairing}\n"
                response += "\n"
            
            return response
        else:
            return "No encontr√© vinos espec√≠ficos para tu consulta. ¬øPodr√≠as darme m√°s detalles sobre qu√© tipo de vino buscas? Por ejemplo: color, regi√≥n, ocasi√≥n o presupuesto."
    
    try:
        # Contexto del usuario
        context_info = ""
        if user_context.get("recent_conversations"):
            context_info = f"\nContexto del usuario: Ha consultado recientemente sobre {len(user_context['recent_conversations'])} temas."
        
        if user_context.get("preferences"):
            prefs = user_context["preferences"]
            context_info += f"\nPreferencias del usuario: {json.dumps(prefs, ensure_ascii=False)}"
        
        # Prompt seg√∫n categor√≠a - M√ÅS DETALLADO
        if category == "WINE_SEARCH":
            system_prompt = """Eres Sumy, un sumiller experto y apasionado. Tu objetivo es proporcionar recomendaciones de vinos detalladas y √∫tiles.

ESTILO DE RESPUESTA:
- S√© profesional pero cercano y entusiasta
- Proporciona informaci√≥n completa y pr√°ctica
- Incluye detalles sobre caracter√≠sticas organol√©pticas cuando sea relevante
- Explica por qu√© recomiendas cada vino
- A√±ade consejos de servicio o maridaje cuando sea apropiado
- Usa emojis ocasionalmente para hacer la respuesta m√°s amigable

ESTRUCTURA RECOMENDADA:
1. Saludo breve y contextualizaci√≥n
2. Recomendaciones espec√≠ficas (1-3 vinos)
3. Detalles de cada vino (nombre, precio, regi√≥n, caracter√≠sticas)
4. Razones de la recomendaci√≥n
5. Consejos adicionales si es relevante"""
            
            wines_info = json.dumps(wines, indent=2, ensure_ascii=False) if wines else "No se encontraron vinos espec√≠ficos"
            user_content = f"""Consulta del cliente: "{query}"

Vinos disponibles en nuestra carta:
{wines_info}

{context_info}

Proporciona una recomendaci√≥n completa y profesional como sumiller experto. Incluye detalles sobre las caracter√≠sticas de los vinos, por qu√© los recomiendas para esta consulta espec√≠fica, y cualquier consejo adicional que pueda ser √∫til."""

        elif category == "WINE_THEORY":
            system_prompt = """Eres Sumy, un sumiller experto con amplio conocimiento en enolog√≠a y viticultura. Tu objetivo es educar y compartir conocimiento de manera clara y completa.

ESTILO DE RESPUESTA:
- Proporciona explicaciones claras y completas
- Usa ejemplos pr√°cticos cuando sea posible
- Incluye contexto hist√≥rico o t√©cnico relevante
- Conecta la teor√≠a con la pr√°ctica
- S√© did√°ctico pero no condescendiente
- Incluye consejos pr√°cticos para aplicar el conocimiento

ESTRUCTURA RECOMENDADA:
1. Explicaci√≥n del concepto principal
2. Detalles t√©cnicos relevantes
3. Ejemplos pr√°cticos o casos de uso
4. Consejos para la aplicaci√≥n pr√°ctica
5. Recomendaciones adicionales si es apropiado"""
            
            user_content = f"""Consulta sobre enolog√≠a/viticultura: "{query}"

{context_info}

Proporciona una explicaci√≥n completa y educativa como sumiller experto. Incluye tanto aspectos t√©cnicos como pr√°cticos, y conecta la informaci√≥n con experiencias reales de cata o servicio."""
        
        else:
            # Para otras categor√≠as, usar respuesta predefinida pero m√°s completa
            return CATEGORY_RESPONSES.get(category, "Como sumiller profesional, me especializo en el mundo del vino. Puedo ayudarte con recomendaciones de vinos, maridajes, t√©cnicas de cata, informaci√≥n sobre regiones vitivin√≠colas, y mucho m√°s. ¬øEn qu√© aspecto del vino te gustar√≠a que te asesore?")
        
        response = await openai_client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            temperature=0.7,
            max_tokens=800  # Aumentado significativamente para respuestas m√°s completas
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        logger.error(f"Error generando respuesta IA: {e}")
        # Fallback inteligente - M√ÅS COMPLETO
        if wines:
            top_wines = wines[:3]
            response = "üç∑ **Recomendaciones basadas en tu consulta:**\n\n"
            
            for i, wine in enumerate(top_wines, 1):
                name = wine.get("name", "Vino")
                price = wine.get("price", "N/A")
                region = wine.get("region", "")
                wine_type = wine.get("type", "")
                description = wine.get("description", "")
                
                response += f"**{i}. {name}**\n"
                if wine_type:
                    response += f"   ‚Ä¢ Tipo: {wine_type}\n"
                if region:
                    response += f"   ‚Ä¢ Regi√≥n: {region}\n"
                if price != "N/A":
                    response += f"   ‚Ä¢ Precio: {price}‚Ç¨\n"
                if description:
                    response += f"   ‚Ä¢ Caracter√≠sticas: {description[:200]}{'...' if len(description) > 200 else ''}\n"
                
                # A√±adir raz√≥n espec√≠fica basada en la consulta
                query_lower = query.lower()
                if any(word in query_lower for word in ["pescado", "mariscos", "sushi"]):
                    response += f"   ‚Ä¢ ¬øPor qu√© te lo recomiendo?: Ideal para pescados y mariscos por su frescura y acidez equilibrada\n"
                elif any(word in query_lower for word in ["carne", "cordero", "ternera", "asado"]):
                    response += f"   ‚Ä¢ ¬øPor qu√© te lo recomiendo?: Perfecto para carnes por su estructura t√°nica y cuerpo\n"
                elif any(word in query_lower for word in ["queso", "tabla"]):
                    response += f"   ‚Ä¢ ¬øPor qu√© te lo recomiendo?: Excelente con quesos por su equilibrio y complejidad\n"
                else:
                    response += f"   ‚Ä¢ ¬øPor qu√© te lo recomiendo?: Recomendado por su excelente relaci√≥n calidad-precio y versatilidad\n"
                
                response += "\n"
            
            response += "üí° **Consejo del sumiller**: Sirve a la temperatura adecuada y considera decantar si es un tinto con cuerpo para potenciar sus aromas."
            return response
        else:
            return "No encontr√© vinos espec√≠ficos para tu consulta, pero estar√© encantado de ayudarte. ¬øPodr√≠as contarme m√°s detalles? Por ejemplo: ¬øqu√© tipo de vino prefieres (tinto, blanco, rosado)?, ¬øpara qu√© ocasi√≥n?, ¬øtienes alg√∫n presupuesto en mente?, ¬øhay alguna regi√≥n que te interese especialmente?"

# ‚ú® FUNCI√ìN MEJORADA: Generar respuesta del sumiller usando RAG + IA
async def generate_sumiller_response_with_rag(
    query: str, 
    rag_response: Dict[str, Any], 
    user_context: Dict[str, Any],
    category: str = "WINE_THEORY"
) -> str:
    """Generar respuesta del sumiller combinando RAG con OpenAI."""
    
    if not openai_client:
        # Respuesta fallback usando solo RAG - M√ÅS COMPLETA
        if rag_response.get("answer"):
            return rag_response["answer"]
        elif rag_response.get("sources"):
            response = "üìö **Bas√°ndome en mi conocimiento especializado:**\n\n"
            
            for i, source in enumerate(rag_response["sources"][:3], 1):
                content = source.get("content", "")
                metadata = source.get("metadata", {})
                
                if metadata.get("type") == "vino":
                    response += f"**{i}. {metadata.get('name', 'Vino')}**\n"
                    response += f"   ‚Ä¢ Regi√≥n: {metadata.get('region', 'N/A')}\n"
                    response += f"   ‚Ä¢ Precio: {metadata.get('price', 'N/A')}‚Ç¨\n"
                    response += f"   ‚Ä¢ Caracter√≠sticas: {content[:300]}{'...' if len(content) > 300 else ''}\n\n"
                else:
                    response += f"**Informaci√≥n relevante {i}:**\n"
                    response += f"{content[:400]}{'...' if len(content) > 400 else ''}\n\n"
            
            return response
        else:
            return "No encontr√© informaci√≥n espec√≠fica para tu consulta en mi base de conocimientos. ¬øPodr√≠as reformular la pregunta o ser m√°s espec√≠fico sobre qu√© aspecto del vino te interesa?"
    
    try:
        # Extraer informaci√≥n del RAG
        rag_sources = rag_response.get("sources", [])
        rag_content = ""
        
        if rag_sources:
            rag_content = "Informaci√≥n especializada de mi base de conocimientos:\n\n"
            for i, source in enumerate(rag_sources[:3], 1):
                content = source.get("content", "")
                metadata = source.get("metadata", {})
                
                rag_content += f"Fuente {i}:\n"
                rag_content += f"Tipo: {metadata.get('type', 'informaci√≥n')}\n"
                if metadata.get("name"):
                    rag_content += f"Nombre: {metadata.get('name')}\n"
                rag_content += f"Contenido: {content[:500]}{'...' if len(content) > 500 else ''}\n\n"
        
        # Contexto del usuario
        context_info = ""
        if user_context.get("recent_conversations"):
            context_info = f"Contexto del usuario: Ha consultado recientemente sobre {len(user_context['recent_conversations'])} temas relacionados con vinos."
        
        if user_context.get("preferences"):
            prefs = user_context["preferences"]
            context_info += f"\nPreferencias conocidas: {json.dumps(prefs, ensure_ascii=False)}"
        
        # Prompt especializado para combinar RAG + sumiller - M√ÅS DETALLADO
        system_prompt = """Eres Sumy, un sumiller profesional con acceso a una extensa base de conocimientos especializada en vinos y enolog√≠a.

Tu objetivo es proporcionar respuestas completas, educativas y pr√°cticas que combinen:
1. La informaci√≥n t√©cnica y espec√≠fica de tu base de conocimientos
2. Tu experiencia profesional como sumiller
3. Consejos pr√°cticos y aplicables
4. Recomendaciones personalizadas

ESTILO DE RESPUESTA:
- Combina informaci√≥n t√©cnica con experiencia pr√°ctica
- S√© detallado pero accesible
- Incluye ejemplos concretos cuando sea posible
- Proporciona consejos pr√°cticos de servicio, cata o maridaje
- Conecta conceptos te√≥ricos con aplicaciones reales
- Usa un tono profesional pero cercano y entusiasta

ESTRUCTURA RECOMENDADA:
1. Introducci√≥n contextualizada a la consulta
2. Informaci√≥n principal basada en tu base de conocimientos
3. An√°lisis y explicaci√≥n profesional
4. Recomendaciones espec√≠ficas o consejos pr√°cticos
5. Sugerencias adicionales o pr√≥ximos pasos"""
        
        user_content = f"""Consulta del cliente: "{query}"

{rag_content}

{context_info}

Proporciona una respuesta completa y profesional como sumiller experto. Combina la informaci√≥n de tu base de conocimientos con tu experiencia pr√°ctica, y aseg√∫rate de incluir consejos √∫tiles y aplicables."""

        response = await openai_client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            temperature=0.7,
            max_tokens=1000  # Aumentado significativamente para respuestas m√°s completas
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        logger.error(f"Error generando respuesta RAG+IA: {e}")
        # Fallback usando solo RAG - M√ÅS COMPLETO
        if rag_response.get("sources"):
            response = "üìö **Bas√°ndome en mi conocimiento especializado:**\n\n"
            
            for i, source in enumerate(rag_response["sources"][:3], 1):
                content = source.get("content", "")
                metadata = source.get("metadata", {})
                
                if metadata.get("type") == "vino":
                    response += f"**{i}. {metadata.get('name', 'Vino recomendado')}**\n"
                    if metadata.get("wine_type"):
                        response += f"   ‚Ä¢ Tipo: {metadata.get('wine_type')}\n"
                    if metadata.get("region"):
                        response += f"   ‚Ä¢ Regi√≥n: {metadata.get('region')}\n"
                    if metadata.get("price"):
                        response += f"   ‚Ä¢ Precio: {metadata.get('price')}‚Ç¨\n"
                    if metadata.get("pairing"):
                        response += f"   ‚Ä¢ Maridaje: {metadata.get('pairing')}\n"
                    response += f"   ‚Ä¢ Descripci√≥n: {content[:300]}{'...' if len(content) > 300 else ''}\n\n"
                else:
                    response += f"**Informaci√≥n t√©cnica {i}:**\n"
                    response += f"{content[:400]}{'...' if len(content) > 400 else ''}\n\n"
            
            response += "\nüí° **Como sumiller te recomiendo**: Considera estos factores al elegir y servir el vino para obtener la mejor experiencia."
            return response
        else:
            return "Como sumiller profesional, puedo ayudarte con cualquier consulta sobre vinos. Mi especialidad incluye recomendaciones personalizadas, t√©cnicas de cata, maridajes, informaci√≥n sobre regiones vitivin√≠colas, y consejos de servicio. ¬øEn qu√© aspecto espec√≠fico del mundo del vino te gustar√≠a que te asesore?"

# üÜï ENDPOINT PRINCIPAL CON FILTRO INTELIGENTE
@app.post("/query", response_model=SumillerResponse)
async def sumiller_query_with_filter(request: QueryRequest = Body(...)):
    """
    ‚ú® ENDPOINT PRINCIPAL CON FILTRO INTELIGENTE
    Clasifica consultas antes de decidir si usar b√∫squeda o respuesta directa
    """
    try:
        # Inicializar memoria si es necesario (para tests)
        if memory is None:
            temp_memory = SumillerMemory()
        else:
            temp_memory = memory
        
        # 1. DETECTAR MENSAJE SECRETO (√∫nica clasificaci√≥n que mantenemos)
        query_lower = request.query.lower()
        secret_triggers = [
            "pedro", "pedrito", "pepe", "perico", 
            "vicky", "victoria", "vicki", "vic"
        ]
        if any(trigger in query_lower for trigger in secret_triggers):
            category = "SECRET_MESSAGE"
            response_text = CATEGORY_RESPONSES.get("SECRET_MESSAGE", "ü§´ Mensaje secreto detectado...")
            wines_recommended = []
            used_rag = False
            confidence = 1.0
            user_context = await temp_memory.get_user_context(request.user_id)
        else:
            # 2. SIEMPRE USAR RAG - Dejar que el RAG decida qu√© contestar
            logger.info("ü§ñ Consultando RAG service para cualquier consulta...")
            
            # Obtener contexto del usuario
            user_context = await temp_memory.get_user_context(request.user_id)
            
            # Consultar RAG service
            rag_response = await query_rag_service(request.query, user_context)
            
            wines_recommended = []
            used_rag = True
            category = "RAG_RESPONSE"
            confidence = 0.8
            
            if rag_response.get("sources") and not rag_response.get("error"):
                # Procesar fuentes de vinos si las hay
                logger.info(f"üçá Procesando {len(rag_response['sources'])} fuentes RAG...")
                
                for i, src in enumerate(rag_response["sources"]):
                    logger.info(f"üîç Fuente {i+1}: {src.get('metadata', {})}")
                    
                    if src.get("metadata", {}).get("type") in ["wine", "vino"]:
                        wine_data = src["metadata"].copy()
                        wine_data["relevance_score"] = src.get("relevance_score", 0)
                        
                        # Extraer descripci√≥n del contenido
                        content = src.get("content", "")
                        if "Descripci√≥n: " in content:
                            description = content.split("Descripci√≥n: ")[1].split("\n")[0]
                            wine_data["description"] = description
                        elif content:
                            lines = content.split("\n")
                            description_lines = [line for line in lines if line.strip() and not line.startswith("Vino:")]
                            if description_lines:
                                wine_data["description"] = description_lines[0][:200]
                        
                        wines_recommended.append(wine_data)
                        logger.info(f"‚úÖ Vino a√±adido: {wine_data.get('name', 'Unknown')}")
                
                logger.info(f"üç∑ Total vinos encontrados: {len(wines_recommended)}")
                
                # Generar respuesta inteligente basada en el contenido RAG
                if wines_recommended:
                    # Si hay vinos, generar respuesta de recomendaci√≥n
                    response_text = await generate_sumiller_response(
                        request.query, wines_recommended, user_context, "WINE_SEARCH"
                    )
                elif rag_response.get("answer"):
                    # Si hay respuesta te√≥rica del RAG, usarla
                    response_text = await generate_sumiller_response_with_rag(
                        request.query, rag_response, user_context, "WINE_THEORY"
                    )
                else:
                    # Respuesta general basada en las fuentes disponibles
                    response_text = await generate_sumiller_response(
                        request.query, [], user_context, "GENERAL"
                    )
            else:
                # Si RAG falla, respuesta general del sumiller
                logger.warning(f"‚ö†Ô∏è RAG fall√≥: {rag_response.get('error', 'Unknown error')}")
                response_text = await generate_sumiller_response(
                    request.query, [], user_context, "GENERAL"
                )
                used_rag = False
        
        # 3. GUARDAR INTERACCI√ìN EN MEMORIA
        await temp_memory.save_conversation(
            request.user_id,
            request.query,
            response_text,
            wines_recommended,
            request.session_id
        )
        
        # 4. RESPUESTA FINAL
        return SumillerResponse(
            response=response_text,
            wines_recommended=wines_recommended,
            user_context=user_context,
            confidence=confidence,
            query_category=category,
            used_rag=used_rag
        )
        
    except Exception as e:
        logger.error(f"Error en consulta: {e}")
        raise HTTPException(status_code=500, detail=f"Error procesando consulta: {str(e)}")

@app.post("/rate-wine")
async def rate_wine(request: WineRatingRequest = Body(...)):
    """Valorar un vino."""
    try:
        # Inicializar memoria si es necesario
        temp_memory = memory if memory is not None else SumillerMemory()
        
        await temp_memory.rate_wine(
            request.user_id,
            request.wine_name,
            request.rating,
            request.notes
        )
        
        return {
            "message": f"Vino '{request.wine_name}' valorado con {request.rating}/5 estrellas",
            "user_id": request.user_id
        }
        
    except Exception as e:
        logger.error(f"Error valorando vino: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/preferences")
async def update_preferences(request: PreferencesRequest = Body(...)):
    """Actualizar preferencias del usuario."""
    try:
        temp_memory = memory if memory is not None else SumillerMemory()
        
        await temp_memory.update_preferences(request.user_id, request.preferences)
        
        return {
            "message": f"Preferencias actualizadas para {request.user_id}",
            "preferences": request.preferences
        }
        
    except Exception as e:
        logger.error(f"Error actualizando preferencias: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/user/{user_id}/context")
async def get_user_context(user_id: str):
    """Obtener contexto del usuario."""
    try:
        temp_memory = memory if memory is not None else SumillerMemory()
        context = await temp_memory.get_user_context(user_id)
        return context
        
    except Exception as e:
        logger.error(f"Error obteniendo contexto: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """Health check del servicio."""
    try:
        # Verificar memoria
        temp_memory = memory if memory is not None else SumillerMemory()
        memory_stats = await temp_memory.get_stats()
        
        # Verificar OpenAI
        ai_status = "ok" if openai_client else "unavailable"
        
        # Estado general
        status = "healthy" if (memory_stats and ai_status == "ok") else "degraded"
        
        return {
            "status": status,
            "service": "sumiller-service",
            "version": "2.0.0",
            "timestamp": datetime.now().isoformat(),
            "components": {
                "database": "ok" if memory_stats else "error",
                "ai_service": ai_status,
                "wine_search": "ok"
            },
            "memory_stats": memory_stats
        }
        
    except Exception as e:
        logger.error(f"Error en health check: {e}")
        return {
            "status": "error",
            "service": "sumiller-service",
            "version": "2.0.0",
            "timestamp": datetime.now().isoformat(),
            "error": str(e)
        }

@app.get("/stats")
async def get_stats():
    """Estad√≠sticas del servicio."""
    try:
        temp_memory = memory if memory is not None else SumillerMemory()
        
        # Intentar obtener estad√≠sticas de memoria
        try:
            memory_stats = await temp_memory.get_stats()
        except Exception as memory_error:
            logger.warning(f"Error obteniendo stats de memoria: {memory_error}")
            memory_stats = {
                "error": "Memory stats unavailable",
                "total_conversations": 0,
                "total_users": 0,
                "total_ratings": 0
            }
        
        return {
            "service": "sumiller-service",
            "version": "2.0.0",
            "memory": memory_stats,
            "wine_database": {
                "total_wines": len(WINE_KNOWLEDGE),
                "regions": list(set(wine["region"] for wine in WINE_KNOWLEDGE)),
                "types": list(set(wine["type"] for wine in WINE_KNOWLEDGE))
            },
            "features": {
                "intelligent_filter": True,
                "memory_system": True,
                "wine_search": True,
                "ai_responses": openai_client is not None
            }
        }
        
    except Exception as e:
        logger.error(f"Error obteniendo estad√≠sticas: {e}")
        # En lugar de lanzar excepci√≥n, devolver stats b√°sicas
        return {
            "service": "sumiller-service",
            "version": "2.0.0",
            "error": str(e),
            "wine_database": {
                "total_wines": len(WINE_KNOWLEDGE),
                "regions": list(set(wine["region"] for wine in WINE_KNOWLEDGE)),
                "types": list(set(wine["type"] for wine in WINE_KNOWLEDGE))
            },
            "features": {
                "intelligent_filter": True,
                "memory_system": False,  # Error en memoria
                "wine_search": True,
                "ai_responses": openai_client is not None
            }
        }

# üÜï ENDPOINT PARA PROBAR CLASIFICACI√ìN
@app.post("/classify")
async def test_classification(request: QueryRequest = Body(...)):
    """Endpoint para probar la clasificaci√≥n de consultas."""
    try:
        if not openai_client:
            return {"error": "OpenAI no configurado"}
        
        classification = await filter_and_classify_query(openai_client, request.query)
        
        return {
            "query": request.query,
            "classification": classification,
            "predefined_response": CATEGORY_RESPONSES.get(classification["category"], "Sin respuesta predefinida")
        }
        
    except Exception as e:
        logger.error(f"Error en clasificaci√≥n: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    logger.info(f"üç∑ Iniciando Sumiller Service en {HOST}:{PORT}")
    uvicorn.run(app, host=HOST, port=PORT)